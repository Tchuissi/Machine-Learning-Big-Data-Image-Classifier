{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_Download_Images_To_Google_Drive.ipynb","provenance":[],"authorship_tag":"ABX9TyObFg+GfoxQ3HfXLo44Upx2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"glNgkxDSuVzw"},"source":["### Get IP Address"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyycdwNUehgR","executionInfo":{"status":"ok","timestamp":1620601901702,"user_tz":240,"elapsed":709,"user":{"displayName":"Barker French","photoUrl":"","userId":"04269807909517149242"}},"outputId":"7c623acc-8c0f-4694-bcff-983d75415116"},"source":["# Get the ip address of the colab notebook to add to MongoDB\n","## This is a security measure that limits calls to approved ip addresses\n","!curl ipecho.net/plain"],"execution_count":3,"outputs":[{"output_type":"stream","text":["35.237.62.88"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xx0XouyCeZFb"},"source":["### Import Libraries"]},{"cell_type":"code","metadata":{"id":"bWBgrEDldrGk"},"source":["!pip install langdetect\n","!pip install -q findspark\n","!sudo apt update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n","!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","import pprint\n","import requests\n","import json\n","from langdetect import detect\n","from langdetect import detect_langs\n","import nltk\n","nltk.download('punkt')\n","import string\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n","import findspark\n","findspark.init()\n","import pyspark\n","# The below import allows the creation of the Spark Session\n","from pyspark.sql import SparkSession\n","from google.colab import drive\n","drive.mount('/content/gdrive/',force_remount=True)\n","import sys\n","drive_path = '/content/gdrive/My Drive/603 Platform for Big Data Processing-Group Project/'\n","sys.path.append(drive_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GPQMKIbPuSgt"},"source":["### Connect to Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9jxwN1ReFOe","executionInfo":{"elapsed":22995,"status":"ok","timestamp":1620489408812,"user":{"displayName":"Barker French","photoUrl":"","userId":"04269807909517149242"},"user_tz":240},"outputId":"9d0eb39c-75e0-4f43-ee9e-5b22dcbe6f1b"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/',force_remount=True)\n","import sys\n","drive_path = '/content/gdrive/My Drive/603 Platform for Big Data Processing-Group Project/'\n","sys.path.append(drive_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BncAXJUOi25B"},"source":["### Create Spark Session"]},{"cell_type":"code","metadata":{"id":"yQYEWu9TFKEO"},"source":["#https://www.mongodb.com/blog/post/getting-started-with-mongodb-pyspark-and-jupyter-notebook\n","#https://docs.mongodb.com/spark-connector/current/python-api/ \n","\n","spark = SparkSession \\\n","        .builder \\\n","        .appName(\"pyspark-notebook2\") \\\n","        .config(\"spark.mongodb.input.uri\",\"mongodb+srv://Barker:$Bs4Op!*k@data603project.3vm61.mongodb.net/flickr_db.pic_records?retryWrites=true&w=majority\") \\\n","        .config(\"spark.mongodb.output.uri\",\"mongodb+srv://Barker:$Bs4Op!*k@data603project.3vm61.mongodb.net/flickr_db.pic_records?retryWrites=true&w=majority\") \\\n","        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\") \\\n","        .getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAwsHdFFqICV"},"source":["## Load new data to train model"]},{"cell_type":"markdown","metadata":{"id":"VgOEF0JMqSUj"},"source":["Loading data from MongoDB Atlas"]},{"cell_type":"code","metadata":{"id":"YUG0GFXEHutt"},"source":["# Reads the records from the database\n","df_collection = spark.read.format(\"mongo\").option(\"database\",\n","\"flickr_db\").option(\"collection\", \"pic_records\").load()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dChASKq0qV88"},"source":["Saving to json"]},{"cell_type":"code","metadata":{"id":"BCEal1Hrrgu8"},"source":["import json\n","\n","df_collection = df_collection.drop('_id')\n","py_json = df_collection.toJSON()\n","new_data = []\n","for row in py_json.collect():\n","    #json string\n","    new_data.append(json.loads(row))\n","\n","with open(f'{drive_path}data/training_data/train.json', 'w') as json_file:\n","  json.dump(new_data, json_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8reH2XfveAy"},"source":["### Download Images to Training Data"]},{"cell_type":"code","metadata":{"id":"xhyCWGkVwCTq"},"source":["from PIL import Image\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","\n","image_url_domain = 'https://live.staticflickr.com/'\n","\n","for image in new_data:\n","\n","  #sample_image_url = f\"{image_url_domain}/{response['photos']['photo'][0]['server']}/{response['photos']['photo'][0]['id']}_{response['photos']['photo'][0]['secret']}_c.jpg\"\n","  sample_image_url = f\"{image_url_domain}/{image['server']}/{image['filename']}\"\n","  image_request = requests.get(sample_image_url)\n","  try:\n","    _image = Image.open(BytesIO(image_request.content))\n","    _image.save(f'{drive_path}data/training_data/images/{image[\"filename\"]}', 'JPEG')\n","    print(f'saved: {image[\"filename\"]}')\n","  except:\n","    print('error')"],"execution_count":null,"outputs":[]}]}