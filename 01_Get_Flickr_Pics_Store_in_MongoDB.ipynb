{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_Get_Flickr_Pics/Store_in_MongoDB.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrh3uWg3IvBvBGZVQY5HTk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F1Dk2N3t7q-U"},"source":["### Get IP Address for MongoDB"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGy_x7Xt7Er5","executionInfo":{"status":"ok","timestamp":1620601376089,"user_tz":240,"elapsed":831,"user":{"displayName":"Barker French","photoUrl":"","userId":"04269807909517149242"}},"outputId":"b6d5bc4d-26dc-44ee-c0b5-effe910b9bc8"},"source":["# Get the ip address of the colab notebook to add to MongoDB\n","## This is a security measure that limits calls to approved ip addresses\n","!curl ipecho.net/plain"],"execution_count":null,"outputs":[{"output_type":"stream","text":["35.229.53.1"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"73JbWCc-7wvZ"},"source":["### Import Libraries/Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"0QWznBb17hPe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"efd94a94-81da-4388-e200-bd71a72a29a2"},"source":["!pip install langdetect\n","!pip install -q findspark\n","!sudo apt update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n","!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","import pprint\n","import requests\n","import json\n","from langdetect import detect\n","from langdetect import detect_langs\n","import nltk\n","nltk.download('punkt')\n","import string\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n","import findspark\n","findspark.init()\n","import pyspark\n","# The below import allows the creation of the Spark Session\n","from pyspark.sql import SparkSession\n","from google.colab import drive\n","drive.mount('/content/gdrive/',force_remount=True)\n","import sys\n","drive_path = '/content/gdrive/My Drive/603 Platform for Big Data Processing-Group Project/'\n","sys.path.append(drive_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1HWhW3C8BXre"},"source":["### Create a Spark Session that connects to our MongoDB Database"]},{"cell_type":"code","metadata":{"id":"yQYEWu9TFKEO"},"source":["#https://www.mongodb.com/blog/post/getting-started-with-mongodb-pyspark-and-jupyter-notebook\n","#https://docs.mongodb.com/spark-connector/current/python-api/ \n","\n","spark = SparkSession \\\n","        .builder \\\n","        .appName(\"pyspark-notebook2\") \\\n","        .config(\"spark.mongodb.input.uri\",\"mongodb+srv://Barker:$Bs4Op!*k@data603project.3vm61.mongodb.net/flickr_db.pic_records?retryWrites=true&w=majority\") \\\n","        .config(\"spark.mongodb.output.uri\",\"mongodb+srv://Barker:$Bs4Op!*k@data603project.3vm61.mongodb.net/flickr_db.pic_records?retryWrites=true&w=majority\") \\\n","        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\") \\\n","        .getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Em1N9eLHUgGo"},"source":["### Make a call to the Flickr API"]},{"cell_type":"code","metadata":{"id":"OCVwoUwdWxSA"},"source":["def get_flickr_response (): \n","  '''This function makes a REST request to the flickr api and returns the most recent 500 photos posted'''\n","\n","  api_endpoint = 'https://www.flickr.com/services/rest/'\n","  image_url_domain = 'https://live.staticflickr.com/'\n","\n","  query_params = {\n","      'method': 'flickr.photos.getRecent',\n","      'api_key': '7910af896978997a5a8d45af06004200',\n","      'extras': 'tags,description',\n","      'per_page' : 500,\n","      'format': 'json',\n","      'nojsoncallback': 1\n","  }\n","\n","  #make the request\n","  request = requests.get(api_endpoint,params=query_params)\n","  #convert the request to json\n","  response = json.loads(request.text)\n","  #confirm data\n","  print(response['photos']['photo'][1])\n","  return response"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGDjywqqWpmS"},"source":["### Function to update the dictionary"]},{"cell_type":"code","metadata":{"id":"uuHntzfsS6DE"},"source":["def add_image ( pic_dataset : dict, response_1_flickr_image_nested_dict : dict, image_cntr) -> dict:\n","  '''This function takes one flickr_image_nested_dict from the many flickr_image_nested_dictionaries in the \n","  response call to the api that has a description of length greater than 5., which \n","  is also of type <<dict>>,   It then updates the pic_dataset dictionary with the flickr_image data.'''\n","\n","  # Create a data structure in the format required by the Convolutional Neural Network\n","  #pic_dataset = {'dataset': 'flickr',\n","  #             'images': [{'server': '',\n","  #                         'filename': '',\n","  #                         'imgid' : '',\n","  #                         'hasBeenUsedForTraining': False,\n","  #                         'isValidCaption': None,\n","  #                         'sentences' : [{'imgid': '',\n","  #                                         'raw':'',\n","  #                                         'sentid': '',\n","  #                                         'tokens': []}\n","  #                                        ]}]}\n","\n","\n","  #sentence incrementer\n","  sentence_num = 0\n","  # Below code gets the data to enter into the dictionary format needed for the model\n","\n","  # Server location of Pic\n","  server  = response_1_flickr_image_nested_dict['server']\n","\n","  # Concatenation of Filename + Secret(from Flickr) + file extension\n","  filename = response_1_flickr_image_nested_dict['id'] + '_' + response_1_flickr_image_nested_dict['secret'] + '_c.jpg'\n","\n","  # Raw sentence data\n","  raw = response_1_flickr_image_nested_dict['description']['_content']\n","\n","  # Individual Sentences \n","  sentences = nltk.sent_tokenize(raw)\n","\n","  # First sentence num\n","  sentid = sentence_num\n","  #tokens = response_1_flickr_image_nested_dict['description']['_content'].split()\n","\n","  # Populate the Image dictionary\n","  image_dict = {\n","      'server' : server,\n","      'filename' : filename, \n","      'imgid' : image_cntr,\n","      'sentences' : [],\n","      'hasBeenUsedForTraining': False,\n","      'isValidCaption': 0\n","  }\n","  \n","  # Iterate through the sentences\n","  for sentence in sentences:\n","    \n","    # Tokenize the sentences\n","    tokens = nltk.word_tokenize(sentence.lower().translate(str.maketrans('', '', string.punctuation)))\n","\n","    # Store the result in the sentence dictionary\n","    sentence_dict = {\n","        'imgid' : image_cntr,\n","        'raw' : sentence,\n","        'sentid' : sentence_num,\n","        'tokens': tokens\n","     }\n","    \n","    # Append the sentence to the list of dictionaries for each sentenace \n","    image_dict['sentences'].append(sentence_dict)\n","\n","    # Increment the sentence number\n","    sentence_num += 1\n","  # Append the image dictionary to the pic dataset dictionary\n","  pic_dataset['images'].append(image_dict)\n","  return pic_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8tMA2OhrWuhJ"},"source":["### Test for useful description & update dictionary"]},{"cell_type":"code","metadata":{"id":"0gedD4VyLGwt"},"source":["def test_and_format_flickr_image_data (response: dict) -> dict:\n","    # below will be a list of 'pic' data in the model format\n","    pic_dataset = {'dataset' : 'flickr', 'images' : []}\n","    # create a counter for the images \n","    image_cntr = 0\n","    # create a counter for the sentences that describe the images\n","    sentence_counter = 0\n","\n","    # iterate through the photos in the api call\n","    for response_1_flickr_image_nested_dict in response['photos']['photo']:\n","        # isolate the description\n","        desc = response_1_flickr_image_nested_dict['description']['_content']\n","        # test for a description, but remove white space on either side\n","        if len(desc.strip()) > 5:\n","            # Account for failure of language test\n","            try:\n","              # test to confirm that the language is english\n","              if (detect(desc) == 'en'):\n","                # next test the probability that the language is english\n","                if (float(str(detect_langs(desc)[0]).split(':')[1]) > .95):\n","                  # Call the add_image function and assign it to itself\n","                  pic_dataset = add_image ( pic_dataset, response_1_flickr_image_nested_dict,image_cntr)\n","                  # image_counter increments to count the number of images in the <pic_dataset> variable\n","                  image_cntr += 1\n","            except:\n","              pass\n","    return pic_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4fRQLRwc-CQ"},"source":["### Download Flickr Image Data and Store in JSON object"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51Im2emX2P1u","executionInfo":{"status":"ok","timestamp":1620489047743,"user_tz":240,"elapsed":4875,"user":{"displayName":"Barker French","photoUrl":"","userId":"04269807909517149242"}},"outputId":"984ec8c9-60ba-435b-93cd-d44d2a36bd88"},"source":["response = get_flickr_response ()\n","pic_dataset = test_and_format_flickr_image_data (response)\n","json_object = json.dumps(pic_dataset['images'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'id': '51164802172', 'owner': '190471600@N06', 'secret': '73d8015d09', 'server': '65535', 'farm': 66, 'title': 'IMG-20210508-WA0023', 'ispublic': 1, 'isfriend': 0, 'isfamily': 0, 'description': {'_content': ''}, 'tags': ''}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rvo8CagYDT0I"},"source":["### Create Spark DataFrame"]},{"cell_type":"code","metadata":{"id":"DjCRQsclTiTA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620489049350,"user_tz":240,"elapsed":811,"user":{"displayName":"Barker French","photoUrl":"","userId":"04269807909517149242"}},"outputId":"724d545f-fb01-4f16-e3b7-3381c4f123ae"},"source":["#https://stackoverflow.com/questions/49675860/pyspark-converting-json-string-to-dataframe\n","\n","#create a spark dataframe for the json object to upload\n","df_upload = spark.read.json(spark.sparkContext.parallelize([json_object]))\n","# validate the dataframe\n","df_upload.show(5);"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------------------+----------------------+-----+--------------+--------------------+------+\n","|            filename|hasBeenUsedForTraining|imgid|isValidCaption|           sentences|server|\n","+--------------------+----------------------+-----+--------------+--------------------+------+\n","|51164801417_d5055...|                 false|    0|             0|[[0, In front of ...| 65535|\n","|51164805537_98c47...|                 false|    1|             0|[[1, View on Inst...| 65535|\n","|51164807957_ffeb9...|                 false|    2|             0|[[2, Number 47 - ...| 65535|\n","|51164812282_26305...|                 false|    3|             0|[[3, via Burglar ...| 65535|\n","|51164815917_a8365...|                 false|    4|             0|[[4, via <a href=...| 65535|\n","+--------------------+----------------------+-----+--------------+--------------------+------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3ztNwLEK7_yS"},"source":["### Load data to MongoDB"]},{"cell_type":"code","metadata":{"id":"zOUn6mcZBQV3"},"source":["#https://docs.mongodb.com/spark-connector/current/python/write-to-mongodb/\n","df_upload.write.format(\"mongo\").mode(\"append\").option(\"database\",\n","\"flickr_db\").option(\"collection\", \"pic_records\").save()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxfu8xlGgWdE"},"source":[""],"execution_count":null,"outputs":[]}]}